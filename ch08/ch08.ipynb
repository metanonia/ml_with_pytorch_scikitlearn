{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss6WwQxZcyB3"
      },
      "source": [
        "# 머신 러닝 교과서 - 파이토치편"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP5cdCwdLbB1"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-pytorch/blob/main/ch08/ch08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"코랩에서 실행하기\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DwnQU2iLbB4"
      },
      "source": [
        "## 패키지 버전 체크"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYLVzzLFLbB5"
      },
      "source": [
        "check_packages.py 스크립트에서 로드하기 위해 폴더를 추가합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2uYR4JBLbB7",
        "outputId": "4a543c3e-9f93-44fd-bd4d-78fece90edf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-09 07:30:40--  https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1629 (1.6K) [text/plain]\n",
            "Saving to: ‘python_environment_check.py’\n",
            "\n",
            "python_environment_ 100%[===================>]   1.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-09 07:30:40 (24.2 MB/s) - ‘python_environment_check.py’ saved [1629/1629]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# 코랩의 경우 깃허브 저장소로부터 python_environment_check.py를 다운로드 합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
        "else:\n",
        "    sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU1w1ZcGLbB-"
      },
      "source": [
        "권장 패키지 버전을 확인하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT5QZ4MTLbCA",
        "outputId": "82a809fe-32f9-4feb-d5f8-7ef58c6eca5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Your Python version is 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "[FAIL]: pyprind is not installed and/or cannot be imported.\n",
            "[OK] numpy 1.23.5\n",
            "[OK] pandas 1.5.3\n",
            "[OK] sklearn 1.2.2\n",
            "[OK] nltk 3.8.1\n"
          ]
        }
      ],
      "source": [
        "from python_environment_check import check_packages\n",
        "\n",
        "\n",
        "d = {\n",
        "    'numpy': '1.21.2',\n",
        "    'pandas': '1.3.2',\n",
        "    'sklearn': '1.0',\n",
        "    'pyprind': '2.11.3',\n",
        "    'nltk': '3.6',\n",
        "}\n",
        "check_packages(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTv_uMjxcyB4"
      },
      "source": [
        "# 8장 - 감성 분석에 머신 러닝 적용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ss0AF_gcyB5"
      },
      "source": [
        "### 목차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkpNAjXtcyB5"
      },
      "source": [
        "- 텍스트 처리용 IMDb 영화 리뷰 데이터 준비\n",
        "    - 영화 리뷰 데이터셋 구하기\n",
        "    -영화 리뷰 데이터셋을 더 간편한 형태로 전처리\n",
        "- BoW 모델 소개\n",
        "    - 단어를 특성 벡터로 변환\n",
        "    - tf-idf를 사용하여 단어 적합성 평가\n",
        "    - 텍스트 데이터 정제\n",
        "    - 문서를 토큰으로 나누기\n",
        "- 문서 분류를 위한 로지스틱 회귀 모델 훈련\n",
        "- 대용량 데이터 처리: 온라인 알고리즘과 외부 메모리 학습\n",
        "- 잠재 디리클레 할당을 사용한 토픽 모델링\n",
        "    - LDA를 사용한 텍스트 문서 분해\n",
        "    - 사이킷런의 LDA\n",
        "- 요약"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqHmARyScyB6"
      },
      "source": [
        "# 텍스트 처리용 IMDb 영화 리뷰 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agsmeXazcyB6"
      },
      "source": [
        "## 영화 리뷰 데이터셋 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMsd8qjbcyB6"
      },
      "source": [
        "IMDB 영화 리뷰 데이터셋은 [http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)에서 내려받을 수 있습니다. 다운로드된 후 파일 압축을 해제합니다.\n",
        "\n",
        "A) 리눅스(Linux)나 macOS를 사용하면 새로운 터미널(Terminal) 윈도우를 열고 `cd` 명령으로 다운로드 디렉터리로 이동하여 다음 명령을 실행하세요.\n",
        "\n",
        "`tar -zxf aclImdb_v1.tar.gz`\n",
        "\n",
        "B) 윈도(Windows)를 사용하면 7-Zip(http://www.7-zip.org) 같은 무료 압축 유틸리티를 설치하여 다운로드한 파일의 압축을 풀 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Y2_kwrcyB6"
      },
      "source": [
        "**다음처럼 파이썬에서 다운로드하고 압축을 풀 수도 있습니다:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOs3FJPTcyB7",
        "outputId": "cf1b0b89-7e10-4add-9fb7-2b7ec185d604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% | 80 MB | 2.16 MB/s | 37 sec elapsed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'\n",
        "\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "    urllib.request.urlretrieve(source, target, reporthook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WB9vcQJ9cyB7"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir('aclImdb'):\n",
        "\n",
        "    with tarfile.open(target, 'r:gz') as tar:\n",
        "        tar.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhTLvtAdcyB7"
      },
      "source": [
        "## 영화 리뷰 데이터셋을 더 간편한 형태로 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "729l0colcyB7"
      },
      "source": [
        "`pyprind`는 주피터 노트북에서 진행바를 출력하기 위한 유틸리티입니다. `pyprind` 패키지를 설치하려면 다음 셀을 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NUkjwFLcyB7",
        "outputId": "9917cbc7-a028-4812-f67d-6fbf12b22ecb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyprind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xcfgFFkDcyB7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:32\n"
          ]
        }
      ],
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# `basepath`를 압축 해제된 영화 리뷰 데이터셋이 있는\n",
        "# 디렉토리로 바꾸세요\n",
        "\n",
        "basepath = '../data/aclImdb'\n",
        "\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
        "df = pd.DataFrame()\n",
        "for s in ('test', 'train'):\n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(basepath, s, l)\n",
        "        for file in sorted(os.listdir(path)):\n",
        "            with open(os.path.join(path, file),\n",
        "                      'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "            df = pd.concat([df, pd.DataFrame([[txt, labels[l]]])],\n",
        "                           ignore_index=True)\n",
        "            pbar.update()\n",
        "df.columns = ['review', 'sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of8QKAPocyB8"
      },
      "source": [
        "데이터프레임을 섞습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1b7BhbYxcyB8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "df = df.reindex(np.random.permutation(df.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djnDKKULcyB8"
      },
      "source": [
        "선택사항: 만들어진 데이터를 CSV 파일로 저장합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kyV0QwoXcyB9"
      },
      "outputs": [],
      "source": [
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "7LNPVvAKcyB9",
        "outputId": "876696b2-b5ef-4f0f-b293-0252ecf683e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cR49jwpcyB9",
        "outputId": "8c4b209c-1170-4531-ef31-8428d09bb54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phyGy5GScyB9"
      },
      "source": [
        "---\n",
        "\n",
        "### 노트\n",
        "\n",
        "`movie_data.csv` 파일을 만드는데 문제가 있다면 https://github.com/rickiepark/python-machine-learning-book-3rd-edition/tree/master/ch08/ 에서 zip 압축된 버전을 다운로드할 수 있습니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OISRfu-cyB9"
      },
      "source": [
        "# BoW 모델 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL4sPT17cyB-"
      },
      "source": [
        "## 단어를 특성 벡터로 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmEJ2BPtcyB-"
      },
      "source": [
        "CountVectorizer의 fit_transform 메서드를 호출하여 BoW 모델의 어휘사전을 만들고 다음 세 문장을 희소한 특성 벡터로 변환합니다:\n",
        "1. The sun is shining\n",
        "2. The weather is sweet\n",
        "3. The sun is shining, the weather is sweet, and one and one is two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q5rK2FRicyB-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgjoT97McyB-"
      },
      "source": [
        "어휘 사전의 내용을 출력해 보면 BoW 모델의 개념을 이해하는 데 도움이 됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgYsyO_WcyB-",
        "outputId": "c694f8ef-611f-4231-d009-74663a2b4e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ]
        }
      ],
      "source": [
        "print(count.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4MkGIeccyB-"
      },
      "source": [
        "이전 결과에서 볼 수 있듯이 어휘 사전은 고유 단어와 정수 인덱스가 매핑된 파이썬 딕셔너리에 저장되어 있습니다. 그다음 만들어진 특성 벡터를 출력해 봅시다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqefbeRYcyB_"
      },
      "source": [
        "특성 벡터의 각 인덱스는 CountVectorizer의 어휘 사전 딕셔너리에 저장된 정수 값에 해당됩니다. 예를 들어 인덱스 0에 있는 첫 번째 특성은 ‘and’ 단어의 카운트를 의미합니다. 이 단어는 마지막 문서에만 나타나네요. 인덱스 1에 있는 (특성 벡터의 두 번째 열) 단어 ‘is’는 세 문장에 모두 등장합니다. 특성 벡터의 이런 값들을 단어 빈도(term frequency) 라고도 부릅니다. 문서 d에 등장한 단어 t의 횟수를 *tf (t,d)*와 같이 씁니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKbtk17XcyB_",
        "outputId": "e838f95e-a55a-4e07-d32a-e4813d6bbdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# 각 단어가 해당 문서에 몇번 나타는가를 표시\n",
        "#      and is shining sun sweet the two weather\n",
        "# doc1\n",
        "# doc2\n",
        "# doc3\n",
        "\n",
        "print(bag.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWOx1Z7_cyB_"
      },
      "source": [
        "## tf-idf를 사용하여 단어 적합성 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6heXG7odcyB_"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnWdHBptcyB_"
      },
      "source": [
        "텍스트 데이터를 분석할 때 클래스 레이블이 다른 문서에 같은 단어들이 나타나는 경우를 종종 보게 됩니다. 일반적으로 자주 등장하는 단어는 유용하거나 판별에 필요한 정보를 가지고 있지 않습니다. 이 절에서 특성 벡터에서 자주 등장하는 단어의 가중치를 낮추는 기법인 tf-idf(term frequency-inverse document frequency)에 대해 배우겠습니다. tf-idf는 단어 빈도와 역문서 빈도(inverse document frequency)의 곱으로 정의됩니다:\n",
        "\n",
        "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
        "\n",
        "여기에서 tf(t, d)는 이전 절에서 보았던 단어 빈도입니다. *idf(t, d)*는 역문서 빈도로 다음과 같이 계산합니다:\n",
        "\n",
        "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
        "\n",
        "여기에서 $n_d$는 전체 문서 개수이고 *df(d, t)*는 단어 t가 포함된 문서 d의 개수입니다. 분모에 상수 1을 추가하는 것은 선택 사항입니다. 훈련 샘플에 한 번도 등장하지 않는 단어가 있는 경우 분모가 0이 되지 않게 만듭니다. log는 문서 빈도 *df(d, t)*가 낮을 때 역문서 빈도 값이 너무 커지지 않도록 만듭니다.\n",
        "\n",
        "사이킷런 라이브러리에는 `CountVectorizer` 클래스에서 만든 단어 빈도를 입력받아 tf-idf로 변환하는 `TfidfTransformer` 클래스가 구현되어 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "397IiqLvcyB_",
        "outputId": "f812936a-8129-43b1-aa1b-25e72675e0a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
            "  0.43370786 0.         0.        ]\n",
            " [0.         0.43370786 0.         0.         0.         0.55847784\n",
            "  0.43370786 0.         0.55847784]\n",
            " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
            "  0.29671753 0.25119322 0.19103892]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True,\n",
        "                         norm='l2',\n",
        "                         smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs))\n",
        "      .toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyUxVvv-cyCA"
      },
      "source": [
        "이전 절에서 보았듯이 세 번째 문서에서 단어 ‘is’가 가장 많이 나타났기 때문에 단어 빈도가 가장 컸습니다. 동일한 특성 벡터를 tf-idf로 변환하면 단어 ‘is’는 비교적 작은 tf-idf를 가집니다(0.45). 이 단어는 첫 번째와 두 번째 문서에도 나타나므로 판별에 유용한 정보를 가지고 있지 않을 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9h-50XUcyCA"
      },
      "source": [
        "수동으로 특성 벡터에 있는 각 단어의 tf-idf를 계산해 보면 `TfidfTransformer`가 앞서 정의한 표준 tf-idf 공식과 조금 다르게 계산한다는 것을 알 수 있습니다. 사이킷런에 구현된 역문서 빈도 공식은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuC9xxiTcyCA"
      },
      "source": [
        "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
        "\n",
        "비슷하게 사이킷런에서 계산하는 tf-idf는 앞서 정의한 공식과 조금 다릅니다:\n",
        "\n",
        "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
        "\n",
        "일반적으로 tf-idf를 계산하기 전에 단어 빈도(tf)를 정규화하지만 `TfidfTransformer` 클래스는 tf-idf를 직접 정규화합니다. 사이킷런의 `TfidfTransformer`는 기본적으로 L2 정규화를 적용합니다(norm=’l2’). 정규화되지 않은 특성 벡터 v를 L2-노름으로 나누면 길이가 1인 벡터가 반환됩니다:\n",
        "\n",
        "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
        "\n",
        "TfidfTransformer의 작동 원리를 이해하기 위해 세 번째 문서에 있는 단어 ‘is'의 tf-idf를 예로 들어 계산해 보죠.\n",
        "\n",
        "세 번째 문서에서 단어 ‘is’의 단어 빈도는 3입니다(tf=3). 이 단어는 세 개 문서에 모두 나타나기 때문에 문서 빈도가 3입니다(df=3). 따라서 역문서 빈도는 다음과 같이 계산됩니다:\n",
        "\n",
        "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
        "\n",
        "이제 tf-idf를 계산하기 위해 역문서 빈도에 1을 더하고 단어 빈도를 곱합니다:\n",
        "\n",
        "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuhA5F0mcyCA",
        "outputId": "7d80e1e4-3f86-4fe4-f9ed-1554cd40e2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf-idf of term \"is\" = 3.00\n"
          ]
        }
      ],
      "source": [
        "tf_is = 3\n",
        "n_docs = 3\n",
        "idf_is = np.log((n_docs+1) / (3+1))\n",
        "tfidf_is = tf_is * (idf_is + 1)\n",
        "print('tf-idf of term \"is\" = %.2f' % tfidf_is)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3sEX98ocyCA"
      },
      "source": [
        "세 번째 문서에 있는 모든 단어에 대해 이런 계산을 반복하면 tf-idf 벡터 [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]를 얻습니다. 이 특성 벡터의 값은 앞서 사용했던 TfidfTransformer에서 얻은 값과 다릅니다. tf-idf 계산에서 빠트린 마지막 단계는 다음과 같은 L2-정규화입니다::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrFAsgmdcyCA"
      },
      "source": [
        "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
        "\n",
        "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
        "\n",
        "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL6MVgwPcyCA"
      },
      "source": [
        "결과에서 보듯이 사이킷런의 `TfidfTransformer`에서 반환된 결과와 같아졌습니다. tf-idf 계산 방법을 이해했으므로 다음 절로 넘어가 이 개념을 영화 리뷰 데이터셋에 적용해 보죠."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MJlrxXJcyCB",
        "outputId": "72761088-0ba2-4e8a-b31f-5ebe2913ff42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3.38629436, 3.        , 3.38629436, 1.28768207, 1.28768207,\n",
              "       1.28768207, 2.        , 1.69314718, 1.28768207])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
        "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
        "raw_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIJOmCCTcyCB",
        "outputId": "ced72f72-20d0-40ac-f197-8832a205fc85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.50238645, 0.44507629, 0.50238645, 0.19103892, 0.19103892,\n",
              "       0.19103892, 0.29671753, 0.25119322, 0.19103892])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
        "l2_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY754xCtcyCB"
      },
      "source": [
        "## 텍스트 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N6mpkSpEcyCB",
        "outputId": "d98847c2-2c19-4c30-a5e4-4df9f3bdee4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'is seven.<br /><br />Title (Brazil): Not Available'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[0, 'review'][-50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "COv5qDRwcyCB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\W'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\W'\n",
            "/var/folders/z1/jfjb4fns3ml6wq1v7h5swdnc0000gn/T/ipykernel_14839/1939991203.py:4: SyntaxWarning: invalid escape sequence '\\)'\n",
            "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
            "/var/folders/z1/jfjb4fns3ml6wq1v7h5swdnc0000gn/T/ipykernel_14839/1939991203.py:6: SyntaxWarning: invalid escape sequence '\\W'\n",
            "  text = (re.sub('[\\W]+', ' ', text.lower()) +\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kvJb2z2wcyCC",
        "outputId": "d4bebcf1-3b8a-4cc5-99a0-bd514c2657b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor(df.loc[0, 'review'][-50:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "92Bvy1TWcyCC",
        "outputId": "747b8f4e-e0ef-4f7a-de81-ade93c75244d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'this is a test :) :( :)'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j_jzzuN8cyCC"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_r0KbbWcyCC"
      },
      "source": [
        "## 문서를 토큰으로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kLpd983UcyCC"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol5jOif8cyCC",
        "outputId": "0ec43f93-375b-4fb8-9789-93b529c47f65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer('runners like running and thus they run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF_2aZy4cyCD",
        "outputId": "893d15b6-79bf-4741-88e6-f59771d02a91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_porter('runners like running and thus they run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AokNRHlgcyCD",
        "outputId": "005f72e0-45ff-4125-eea2-41e452d0d733"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/seiwoongchoi/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nnbYs76cyCD",
        "outputId": "aa5c2426-5fc2-44f3-e4f0-cbca8dc8845b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
        "if w not in stop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9h1iZPcyCD"
      },
      "source": [
        "# 문서 분류를 위한 로지스틱 회귀 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxZR-OXGcyCD"
      },
      "source": [
        "GridSearch 속도를 높이기 위해 HTML과 구두점을 삭제합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "D82QePVRcyCD"
      },
      "outputs": [],
      "source": [
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "R-hUDykscyCE"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [None],\n",
        "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                     'clf__C': [1.0, 10.0]},\n",
        "                    {'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [stop, None],\n",
        "                     'vect__tokenizer': [tokenizer],\n",
        "                     'vect__use_idf':[False],\n",
        "                     'vect__norm':[None],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                     'clf__C': [1.0, 10.0]},\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6lOUGvwcyCE"
      },
      "source": [
        "**`n_jobs` 매개변수에 대하여**\n",
        "\n",
        "앞의 코드 예제에서 컴퓨터에 있는 모든 CPU 코어를 사용해 그리드 서치의 속도를 높이려면 (`n_jobs=1` 대신) `n_jobs=-1`로 지정하는 것이 좋습니다. 일부 시스템에서는 멀티프로세싱을 위해 `n_jobs=-1`로 지정할 때 `tokenizer` 와 `tokenizer_porter` 함수의 직렬화에 문제가 발생할 수 있습니다. 이런 경우 `[tokenizer, tokenizer_porter]`를 `[str.split]`로 바꾸어 문제를 해결할 수 있습니다. 다만 `str.split`로 바꾸면 어간 추출을 하지 못합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4urLkAyGcyCE"
      },
      "source": [
        "**코드 실행 시간에 대하여**\n",
        "\n",
        "다음 코드 셀을 실행하면 시스템에 따라 **30~60분 정도 걸릴 수 있습니다**. 매개변수 그리드에서 정의한 대로 2*2*2*3*5 + 2*2*2*3*5 = 240개의 모델을 훈련하기 때문입니다.\n",
        "\n",
        "**코랩을 사용할 경우에도 CPU 코어가 많지 않기 때문에 실행 시간이 오래 걸릴 수 있습니다.**\n",
        "\n",
        "너무 오래 기다리기 어렵다면 데이터셋의 훈련 샘플의 수를 다음처럼 줄일 수 있습니다:\n",
        "\n",
        "    X_train = df.loc[:2500, 'review'].values\n",
        "    y_train = df.loc[:2500, 'sentiment'].values\n",
        "    \n",
        "훈련 세트 크기를 줄이는 것은 모델 성능을 감소시킵니다. 그리드에 지정한 매개변수를 삭제하면 훈련한 모델 수를 줄일 수 있습니다. 예를 들면 다음과 같습니다:\n",
        "\n",
        "    param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                   'vect__stop_words': [stop, None],\n",
        "                   'vect__tokenizer': [tokenizer],\n",
        "                   'clf__penalty': ['l1', 'l2'],\n",
        "                   'clf__C': [1.0, 10.0]},\n",
        "                  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "VUm_76rGcyCE",
        "outputId": "395c3414-76de-4fac-b498-64e168e16427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x12c2258a0&gt;,\n",
              "                                              &lt;function tokenizer_porter at 0x12d94ac00&gt;]},\n",
              "                         {&#x27;...\n",
              "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
              "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
              "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
              "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
              "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
              "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
              "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
              "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
              "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                                                &#x27;itself&#x27;, ...],\n",
              "                                               None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x12c2258a0&gt;],\n",
              "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x12c2258a0&gt;,\n",
              "                                              &lt;function tokenizer_porter at 0x12d94ac00&gt;]},\n",
              "                         {&#x27;...\n",
              "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
              "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
              "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
              "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
              "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
              "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
              "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
              "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
              "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                                                &#x27;itself&#x27;, ...],\n",
              "                                               None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x12c2258a0&gt;],\n",
              "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 TfidfVectorizer(lowercase=False,\n",
              "                                 tokenizer=&lt;function tokenizer at 0x12c2258a0&gt;)),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(C=10.0, solver=&#x27;liblinear&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(lowercase=False, tokenizer=&lt;function tokenizer at 0x12c2258a0&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=10.0, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('vect',\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       ('clf',\n",
              "                                        LogisticRegression(solver='liblinear'))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
              "                          'vect__ngram_range': [(1, 1)],\n",
              "                          'vect__stop_words': [None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x12c2258a0>,\n",
              "                                              <function tokenizer_porter at 0x12d94ac00>]},\n",
              "                         {'...\n",
              "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
              "                                                'our', 'ours', 'ourselves',\n",
              "                                                'you', \"you're\", \"you've\",\n",
              "                                                \"you'll\", \"you'd\", 'your',\n",
              "                                                'yours', 'yourself',\n",
              "                                                'yourselves', 'he', 'him',\n",
              "                                                'his', 'himself', 'she',\n",
              "                                                \"she's\", 'her', 'hers',\n",
              "                                                'herself', 'it', \"it's\", 'its',\n",
              "                                                'itself', ...],\n",
              "                                               None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x12c2258a0>],\n",
              "                          'vect__use_idf': [False]}],\n",
              "             scoring='accuracy', verbose=1)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS5WP6zEcyCF",
        "outputId": "b2a78521-f68c-4678-bfa6-e0f3342495bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "최적의 매개변수 조합: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x12c2258a0>}\n",
            "CV 정확도: 0.897\n"
          ]
        }
      ],
      "source": [
        "print(f'최적의 매개변수 조합: {gs_lr_tfidf.best_params_}')\n",
        "print(f'CV 정확도: {gs_lr_tfidf.best_score_:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9UElDZpcyCF",
        "outputId": "ae86bef1-686e-45dd-9eec-240219103913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 정확도: 0.899\n"
          ]
        }
      ],
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print(f'테스트 정확도: {clf.score(X_test, y_test):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PlVtolxcyCF"
      },
      "source": [
        "####  주석 시작:\n",
        "\n",
        "`gs_lr_tfidf.best_score_`는 k-폴드 교차 검증의 평균 점수입니다. 즉 5-폴드 교차 검증으로 `GridSearchCV` 객체를 훈련하면 `best_score_` 속성은 최상의 모델에 대한 5-폴드 점수의 평균을 반환합니다. 예를 들면 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqnaINsIcyCF",
        "outputId": "5b4708ef-b109-4bca-8560-fe4449a5a604"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.4, 0.2, 0.6, 0.2, 0.4])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(precision=6)\n",
        "y = [np.random.randint(3) for i in range(25)]\n",
        "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
        "\n",
        "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
        "\n",
        "lr = LogisticRegression(random_state=123, multi_class='ovr', solver='lbfgs')\n",
        "cross_val_score(lr, X, y, cv=cv5_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y0TYRZFcyCG"
      },
      "source": [
        "위 코드를 실행하면 클래스 레이블에 해당하는 랜덤한 정수 데이터셋을 만듭니다. 그다음 5-폴드의 인덱스(`cv5_idx`)를 `cross_val_score` 함수에 전달하여 5개의 정확도 점수를 받습니다. 이 점수가 다섯 개의 테스트 폴드에 대한 정확도 값입니다.\n",
        "\n",
        "그다음 `GridSearchCV` 객체를 사용해 동일한 5-폴드 인덱스(`cv5_idx`)를 전달해 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieGoMTv5cyCG",
        "outputId": "34eaf16c-6d23-4c08-c0af-f9e6f6ddba6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV 1/5] END ..................................., score=0.400 total time=   0.0s\n",
            "[CV 2/5] END ..................................., score=0.200 total time=   0.0s\n",
            "[CV 3/5] END ..................................., score=0.600 total time=   0.0s\n",
            "[CV 4/5] END ..................................., score=0.200 total time=   0.0s\n",
            "[CV 5/5] END ..................................., score=0.400 total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)\n",
        "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CSWJmkkcyCG"
      },
      "source": [
        "여기서 볼 수 있듯이 5-폴드에 대한 점수는 `cross_val_score`에서 얻은 것과 정확히 일치합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWOgOEI7cyCG"
      },
      "source": [
        "이제 `fit` 메서드를 호출한 후 `GridSearchCV` 객체의 `best_score_` 속성은 최상의 모델의 평균 정확도를 반환합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQFOhwScyCG",
        "outputId": "46d71aa8-2988-4813-c694-b2e1866a9911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.36000000000000004"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMEbcfN0cyCG"
      },
      "source": [
        "이 점수는 `cross_val_score`로 계산한 평균 정확도와 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwWre_ZocyCH",
        "outputId": "b0e22089-a733-435b-e2ae-f5c8fe5a513f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/Volumes/data/source/ml_with_pytorch_scikitlearn/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.36000000000000004"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=1)\n",
        "cross_val_score(lr, X, y, cv=cv5_idx).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNKUcBuNcyCH"
      },
      "source": [
        "#### 주석 끝\n",
        "\n",
        "<hr>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAaA7vnucyCH"
      },
      "source": [
        "# 대용량 데이터 처리: 온라인 알고리즘과 외부 메모리 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "IbhTytKWcyCH"
      },
      "outputs": [],
      "source": [
        "# 이 셀의 코드는 책에 포함되어 있지 않습니다.\n",
        "# 이전 코드를 실행하지 않고 바로 시작할 수 있도록 편의를 위해 추가했습니다.\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "\n",
        "\n",
        "if not os.path.isfile('movie_data.csv'):\n",
        "    if not os.path.isfile('movie_data.csv.gz'):\n",
        "        print('Please place a copy of the movie_data.csv.gz'\n",
        "              'in this directory. You can obtain it by'\n",
        "              'a) executing the code in the beginning of this'\n",
        "              'notebook or b) by downloading it from GitHub:'\n",
        "              'https://github.com/rickiepark/python-machine-learning-'\n",
        "              'book-3rd-edition/blob/master/ch08/movie_data.csv.gz')\n",
        "    else:\n",
        "        with gzip.open('movie_data.csv.gz', 'rb') as in_f, \\\n",
        "                open('movie_data.csv', 'wb') as out_f:\n",
        "            out_f.write(in_f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b6ZDaEfJcyCH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\W'\n",
            "<>:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\W'\n",
            "/var/folders/z1/jfjb4fns3ml6wq1v7h5swdnc0000gn/T/ipykernel_14839/2241221198.py:13: SyntaxWarning: invalid escape sequence '\\)'\n",
            "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
            "/var/folders/z1/jfjb4fns3ml6wq1v7h5swdnc0000gn/T/ipykernel_14839/2241221198.py:14: SyntaxWarning: invalid escape sequence '\\W'\n",
            "  text = re.sub('[\\W]+', ' ', text.lower()) +\\\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# `stop` 객체를 앞에서 정의했지만 이전 코드를 실행하지 않고\n",
        "# 편의상 여기에서부터 코드를 실행하기 위해 다시 만듭니다.\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # 헤더 넘기기\n",
        "        for line in csv:\n",
        "            text, label = line[:-3], int(line[-2])\n",
        "            yield text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oHQu4x-cyCH",
        "outputId": "33476d8f-d883-4fa9-c919-dfec8eccc0af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
              " 1)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(stream_docs(path='movie_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pDIeRNdWcyCH"
      },
      "outputs": [],
      "source": [
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Q1Nv4vyicyCI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore',\n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None,\n",
        "                         tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Y8mJyKD7cyCI"
      },
      "outputs": [],
      "source": [
        "# from distutils.version import LooseVersion as Version\n",
        "\n",
        "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
        "\n",
        "doc_stream = stream_docs(path='movie_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNGiQGLjcyCI",
        "outputId": "480099de-3cbf-4ed6-a467-a1f326ba4243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No valid output stream.\n"
          ]
        }
      ],
      "source": [
        "import pyprind\n",
        "pbar = pyprind.ProgBar(45)\n",
        "\n",
        "classes = np.array([0, 1])\n",
        "for _ in range(45):\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    pbar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMLgSGMHcyCI",
        "outputId": "6fae016a-b2d3-47da-91e1-1e58d4168174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "정확도: 0.868\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print(f'정확도: {clf.score(X_test, y_test):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "CjpZRaP0cyCI"
      },
      "outputs": [],
      "source": [
        "clf = clf.partial_fit(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUuim6rGcyCI"
      },
      "source": [
        "# 잠재 디리클레 할당을 사용한 토픽 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCoicyiUcyCI"
      },
      "source": [
        "## LDA를 사용한 텍스트 문서 분해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgKVwW-WcyCI"
      },
      "source": [
        "## 사이킷런의 LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Xd8wHrIicyCJ",
        "outputId": "a4a6b327-be6b-4952-df3c-5cd3da9b12db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "x6iBTTXEcyCJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(stop_words='english',\n",
        "                        max_df=.1,\n",
        "                        max_features=5000)\n",
        "X = count.fit_transform(df['review'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Tuj9PLJCcyCJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10,\n",
        "                                random_state=123,\n",
        "                                learning_method='batch')\n",
        "X_topics = lda.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ejgt5FScyCJ",
        "outputId": "7697742f-f79c-4817-addb-6e278c581917"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 5000)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lda.components_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEJ6hcpDcyCJ",
        "outputId": "2ce698d0-1ebc-4e54-c3ce-e1cf0e68a026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "토픽 1:\n",
            "worst minutes awful script stupid\n",
            "토픽 2:\n",
            "family mother father children girl\n",
            "토픽 3:\n",
            "american war dvd music tv\n",
            "토픽 4:\n",
            "human audience cinema art sense\n",
            "토픽 5:\n",
            "police guy car dead murder\n",
            "토픽 6:\n",
            "horror house sex girl woman\n",
            "토픽 7:\n",
            "role performance comedy actor performances\n",
            "토픽 8:\n",
            "series episode war episodes tv\n",
            "토픽 9:\n",
            "book version original read novel\n",
            "토픽 10:\n",
            "action fight guy guys cool\n"
          ]
        }
      ],
      "source": [
        "n_top_words = 5\n",
        "feature_names = count.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f'토픽 {(topic_idx + 1)}:')\n",
        "    print(\" \".join([feature_names[i]\n",
        "                    for i in topic.argsort()\\\n",
        "                        [:-n_top_words - 1:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkj0vL2WcyCJ"
      },
      "source": [
        "각 토픽에서 가장 중요한 단어 다섯 개를 기반으로 LDA가 다음 토픽을 구별했다고 추측할 수 있습니다.\n",
        "\n",
        "1.\t대체적으로 형편없는 영화(실제 토픽 카테고리가 되지 못함)\n",
        "2.\t가족 영화\n",
        "3.\t전쟁 영화\n",
        "4.\t예술 영화\n",
        "5.\t범죄 영화\n",
        "6.\t공포 영화\n",
        "7.\t코미디 영화\n",
        "8.\tTV 쇼와 관련된 영화\n",
        "9.\t소설을 원작으로 한 영화\n",
        "10.\t액션 영화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4xvfj7EcyCJ"
      },
      "source": [
        "카테고리가 잘 선택됐는지 확인하기 위해 공포 영화 카테고리에서 3개 영화의 리뷰를 출력해 보죠(공포 영화는 카테고리 6이므로 인덱스는 5입니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6shgjZfcyCK",
        "outputId": "2f9b5aed-2097-4570-8d99-ac5c298bd046",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "공포 영화 #1:\n",
            "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
            "\n",
            "공포 영화 #2:\n",
            "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
            "\n",
            "공포 영화 #3:\n",
            "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
          ]
        }
      ],
      "source": [
        "horror = X_topics[:, 5].argsort()[::-1]\n",
        "\n",
        "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
        "    print(f'\\n공포 영화 #{(iter_idx + 1)}:')\n",
        "    print(df['review'][movie_idx][:300], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXblpnHrcyCK"
      },
      "source": [
        "앞 코드에서 공포 영화 카테고리 중 최상위 3개의 리뷰에서 300자씩 출력했습니다. 정확히 어떤 영화에 속한 리뷰인지는 모르지만 공포 영화의 리뷰임을 알 수 있습니다(하지만 영화 #2는 1번 카테고리에 속하는 것처럼 보이기도 합니다)."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "ch08.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
